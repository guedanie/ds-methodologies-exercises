{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "|               | actual cat | actual dog |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| predicted cat |         34 |          7 |\n",
    "| predicted dog |         13 |         46 |\n",
    "\n",
    "* In the context of this problem, what is a false positive?\n",
    "* In the context of this problem, what is a false negative?\n",
    "* How would you describe this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The models accuracy is: 80%\n",
      "\n",
      "The models recall is: 72%\n",
      "\n",
      "The models precision is: 83%\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "\n",
    "accuracy = (34 + 46) / (34 + 7 + 13 + 46)\n",
    "\n",
    "# Recall\n",
    "\n",
    "recall = (34) / (34 + 13)\n",
    "\n",
    "# Precision\n",
    "\n",
    "precision = (34) / (34 + 7)\n",
    "\n",
    "print(f'''\n",
    "\n",
    "The models accuracy is: {accuracy:.0%}\n",
    "\n",
    "The models recall is: {recall:.0%}\n",
    "\n",
    "The models precision is: {precision:.0%}\n",
    "\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP: Predicted cat, but it was dog\n",
    "FN: Predicted dog, but it was cat\n",
    "\n",
    "In this model, we are trying to predict if an image is an image of a cat or a dog. For evaluation purposes, we will assume that the cat is the positive variable we are looking for (i.e we either correctly indentify its a cat, or we correctly identify it is not a cat). \n",
    "\n",
    "TP: Predicted cat, and it was cat\n",
    "TN: Predicted not cat, and it was not a cat\n",
    "\n",
    "I would likely use Recall to evualte this model, because we would want to ensure we are getting as many of the positive cases as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cody = pd.read_csv(\"c3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cody.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 4 columns):\n",
      "actual    200 non-null object\n",
      "model1    200 non-null object\n",
      "model2    200 non-null object\n",
      "model3    200 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 6.4+ KB\n"
     ]
    }
   ],
   "source": [
    "cody.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Detect defects in rubber ducks \n",
    "\n",
    "* TP: Predicted a defect and there was a defect\n",
    "* TN: Didn't predict a defect, and no defect was present\n",
    "* FP: Predicted a defect, and there wasn't one\n",
    "* FN: Didn't predict a defect, but there was a defect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cody[\"baseline\"] = cody.actual.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find all the ducks with defect, we want to identify the model that has the highest TP or lowest TN, which means we would like to use a precision evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 recall = 50%\n",
      "model 2 recall = 56%\n",
      "model 3 recall = 81%\n"
     ]
    }
   ],
   "source": [
    "positive = \"Defect\"\n",
    "\n",
    "subset = cody[cody.actual==positive]\n",
    "model1_recall = (subset.model1 == subset.actual).mean()\n",
    "model2_recall = (subset.model2 == subset.actual).mean()\n",
    "model3_recall = (subset.model3 == subset.actual).mean()\n",
    "\n",
    "\n",
    "print(f'model 1 recall = {model1_recall:.0%}')\n",
    "print(f'model 2 recall = {model2_recall:.0%}')\n",
    "print(f'model 3 recall = {model3_recall:.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case - model three has the highest accuracy when it comes to detecting actual positives (or least number of FN), so this is the model i would recommend using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we want to ensure that our predictions are as high as possible, which is why we will then evaluate our model using **precision**, becasue now our objective is to make sure we are not giving away free tickets to Hawaii, in other words, the cost of a *False Positive* is much higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of model 1 is: 80%\n",
      "Precision of model 2 is: 10%\n",
      "Precision of model 3 is: 13%\n"
     ]
    }
   ],
   "source": [
    "positive = \"Defect\"\n",
    "\n",
    "model1 = cody[cody.model1== positive]\n",
    "model1_precision = (model1.model1 == model1.actual).mean()\n",
    "\n",
    "model2 = cody[cody.model2== positive]\n",
    "model2_precision = (model2.model2 == model2.actual).mean()\n",
    "\n",
    "model3 = cody[cody.model3 == positive]\n",
    "model3_precision = (model3.model3 == model3.actual).mean()\n",
    "\n",
    "\n",
    "\n",
    "print(f'Precision of model 1 is: {model1_precision:.0%}')\n",
    "print(f'Precision of model 2 is: {model2_precision:.0%}')\n",
    "print(f'Precision of model 3 is: {model3_precision:.0%}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, model 1 is the best one at predicting True positives, or least number of False Positives, so it is the model I would recommend using for the Haiwaii offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. You are working as a data scientist for Gives You Paws ™, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n",
    "At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paws = pd.read_csv(\"gives_you_paws.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      "actual    5000 non-null object\n",
      "model1    5000 non-null object\n",
      "model2    5000 non-null object\n",
      "model3    5000 non-null object\n",
      "model4    5000 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 195.4+ KB\n"
     ]
    }
   ],
   "source": [
    "paws.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog    3254\n",
       "cat    1746\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paws[\"baseline\"] = \"dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 accuracy = 81%\n",
      "Model 2 accuracy = 63%\n",
      "Model 3 accuracy = 51%\n",
      "Model 4 accuracy = 74%\n",
      "Baseline accuracy = 65%\n"
     ]
    }
   ],
   "source": [
    "df = paws\n",
    "\n",
    "# accuracy -- overall hit rate\n",
    "model1_accuracy = (df.model1 == df.actual).mean()\n",
    "model2_accuracy = (df.model2 == df.actual).mean()\n",
    "model3_accuracy = (df.model3 == df.actual).mean()\n",
    "model4_accuracy = (df.model4 == df.actual).mean()\n",
    "baseline_accuracy = (df.baseline == df.actual).mean()\n",
    "\n",
    "\n",
    "print(f'Model 1 accuracy = {model1_accuracy:.0%}')\n",
    "print(f'Model 2 accuracy = {model2_accuracy:.0%}')\n",
    "print(f'Model 3 accuracy = {model3_accuracy:.0%}')\n",
    "print(f'Model 4 accuracy = {model4_accuracy:.0%}')\n",
    "print(f'Baseline accuracy = {baseline_accuracy:.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of accuracy, model1 and model4 performed better than the base line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive = \"dog\"\n",
    "\n",
    "* TP: Predict dog picture, and it is a dog picture\n",
    "* TN: Predict not a picture of dog (ie.cat) and it is a cat picture\n",
    "* FP: Predict picture of dog, and it wasn't \n",
    "* FN: Predicted not a picture of a dog, and it was a picture of a dog\n",
    "\n",
    "For phase I, the risk of missing out on dog pictures is more costly than the risk of mis-identifying pictures, so we would want to ensure a high number of positive cases, because we would want to maximize the number of dog pictures that are brought in. As such, we use a **recall** evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 recall = 80%\n",
      "model 2 recall = 49%\n",
      "model 3 recall = 51%\n",
      "model 4 recall = 96%\n"
     ]
    }
   ],
   "source": [
    "positive = \"dog\"\n",
    "\n",
    "subset = paws[paws.actual==positive]\n",
    "model1_recall = (subset.model1 == subset.actual).mean()\n",
    "model2_recall = (subset.model2 == subset.actual).mean()\n",
    "model3_recall = (subset.model3 == subset.actual).mean()\n",
    "model4_recall = (subset.model4 == subset.actual).mean()\n",
    "\n",
    "\n",
    "print(f'model 1 recall = {model1_recall:.0%}')\n",
    "print(f'model 2 recall = {model2_recall:.0%}')\n",
    "print(f'model 3 recall = {model3_recall:.0%}')\n",
    "print(f'model 4 recall = {model4_recall:.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4 would be best model for phase I. For phase II, we would now need to make sure that our predicted value is as high as possible, because it would be pretty bad if a cat picture showed up on a dog stream, so we need to ensure we have as little False Positives as possible. In other words, it is better to predict that a dog picture is not a dog picture, than to predict that a cat picture is a dog picture. As such, here we use a **precision evaluation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of model 1 is: 89%\n",
      "Precision of model 2 is: 89%\n",
      "Precision of model 3 is: 66%\n",
      "Precision of model 4 is: 73%\n"
     ]
    }
   ],
   "source": [
    "positive = \"dog\"\n",
    "\n",
    "model1 = paws[paws.model1== positive]\n",
    "model1_precision = (model1.model1 == model1.actual).mean()\n",
    "\n",
    "model2 = paws[paws.model2== positive]\n",
    "model2_precision = (model2.model2 == model2.actual).mean()\n",
    "\n",
    "model3 = paws[paws.model3 == positive]\n",
    "model3_precision = (model3.model3 == model3.actual).mean()\n",
    "\n",
    "model4 = paws[paws.model4 == positive]\n",
    "model4_precision = (model4.model4 == model4.actual).mean()\n",
    "\n",
    "\n",
    "\n",
    "print(f'Precision of model 1 is: {model1_precision:.0%}')\n",
    "print(f'Precision of model 2 is: {model2_precision:.0%}')\n",
    "print(f'Precision of model 3 is: {model3_precision:.0%}')\n",
    "print(f'Precision of model 4 is: {model4_precision:.0%}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, model 1 or model 2 would be better, because they have higher *accurate* predicted positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Predict that an image is a cat or not\n",
    "\n",
    "* TP: Predicted cat image, and it was a cat image\n",
    "* TN: Predicted not a cat image, and it wasn't a cat image\n",
    "* FP: Predicted cat image, and it wasn't a cat image\n",
    "* FN: Predicted not a cat image, and it was a cat image\n",
    "\n",
    "\n",
    "Since our objective is to predict value images, I would use recall on the first phase, and precision on the second phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 recall = 82%\n",
      "model 2 recall = 89%\n",
      "model 3 recall = 51%\n",
      "model 4 recall = 35%\n"
     ]
    }
   ],
   "source": [
    "positive = \"cat\"\n",
    "\n",
    "subset = paws[paws.actual==positive]\n",
    "model1_recall = (subset.model1 == subset.actual).mean()\n",
    "model2_recall = (subset.model2 == subset.actual).mean()\n",
    "model3_recall = (subset.model3 == subset.actual).mean()\n",
    "model4_recall = (subset.model4 == subset.actual).mean()\n",
    "\n",
    "\n",
    "print(f'model 1 recall = {model1_recall:.0%}')\n",
    "print(f'model 2 recall = {model2_recall:.0%}')\n",
    "print(f'model 3 recall = {model3_recall:.0%}')\n",
    "print(f'model 4 recall = {model4_recall:.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2 has teh highest percentage of actual positive values, which would make it ideal for phase 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of model 1 is: 69%\n",
      "Precision of model 2 is: 48%\n",
      "Precision of model 3 is: 36%\n",
      "Precision of model 4 is: 81%\n"
     ]
    }
   ],
   "source": [
    "positive = \"cat\"\n",
    "\n",
    "model1 = paws[paws.model1== positive]\n",
    "model1_precision = (model1.model1 == model1.actual).mean()\n",
    "\n",
    "model2 = paws[paws.model2== positive]\n",
    "model2_precision = (model2.model2 == model2.actual).mean()\n",
    "\n",
    "model3 = paws[paws.model3 == positive]\n",
    "model3_precision = (model3.model3 == model3.actual).mean()\n",
    "\n",
    "model4 = paws[paws.model4 == positive]\n",
    "model4_precision = (model4.model4 == model4.actual).mean()\n",
    "\n",
    "\n",
    "\n",
    "print(f'Precision of model 1 is: {model1_precision:.0%}')\n",
    "print(f'Precision of model 2 is: {model2_precision:.0%}')\n",
    "print(f'Precision of model 3 is: {model3_precision:.0%}')\n",
    "print(f'Precision of model 4 is: {model4_precision:.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4 has the most precision when it comes to guessing True Positives, and would be the best for Phase II."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "* sklearn.metrics.accuracy_score\n",
    "\n",
    "Using sklearn to calculate accuracy for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(paws.actual, paws.model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual      1.0000\n",
       "model1      0.8074\n",
       "model2      0.6304\n",
       "model3      0.5096\n",
       "model4      0.7426\n",
       "baseline    0.6508\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws.apply(lambda col: sklearn.metrics.accuracy_score(paws.actual, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "* sklearn.metrics.precision_score\n",
    "\n",
    "Now that we are working just with dog team, we want to \n",
    "predict precision for all model - where positive is \"dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8900238338440586"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgroup = paws[paws.model1 == \"dog\"]\n",
    "\n",
    "sklearn.metrics.precision_score(subgroup.actual, subgroup.model1, pos_label=\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual      1.000000\n",
       "model1      0.890024\n",
       "model2      0.893177\n",
       "model3      0.659888\n",
       "model4      0.731249\n",
       "baseline    0.650800\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws.apply(lambda col: sklearn.metrics.precision_score(paws.actual, col, pos_label=\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the cat team, where positive is \"cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "actual      1.000000\n",
       "model1      0.689772\n",
       "model2      0.484122\n",
       "model3      0.358347\n",
       "model4      0.807229\n",
       "baseline    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws.apply(lambda col: sklearn.metrics.precision_score(paws.actual, col, pos_label = \"cat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "* sklearn.metrics.recall_score\n",
    "\n",
    "Same as above, we will just be doing the dog model, but now evaluating on recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.803318992009834"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.recall_score(paws.actual, paws.model1, pos_label =\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual      1.000000\n",
       "model1      0.803319\n",
       "model2      0.490781\n",
       "model3      0.508605\n",
       "model4      0.955747\n",
       "baseline    1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws.apply(lambda col: sklearn.metrics.recall_score(paws.actual, col, pos_label = \"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the cat team - where positive is \"cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual      1.000000\n",
       "model1      0.815006\n",
       "model2      0.890607\n",
       "model3      0.511455\n",
       "model4      0.345361\n",
       "baseline    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws.apply(lambda col: sklearn.metrics.recall_score(paws.actual, col, pos_label=\"cat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "\n",
    "* sklearn.metrics.classification_report\n",
    "\n",
    "Built a text report showing the main classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sklearn.metrics.classification_report(paws.actual, paws.model1, output_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8074,\n",
      " 'cat': {'f1-score': 0.7471777369388292,\n",
      "         'precision': 0.6897721764420747,\n",
      "         'recall': 0.8150057273768614,\n",
      "         'support': 1746},\n",
      " 'dog': {'f1-score': 0.8444516233241802,\n",
      "         'precision': 0.8900238338440586,\n",
      "         'recall': 0.803318992009834,\n",
      "         'support': 3254},\n",
      " 'macro avg': {'f1-score': 0.7958146801315047,\n",
      "               'precision': 0.7898980051430666,\n",
      "               'recall': 0.8091623596933477,\n",
      "               'support': 5000},\n",
      " 'weighted avg': {'f1-score': 0.8104835821984157,\n",
      "                  'precision': 0.8200959550792857,\n",
      "                  'recall': 0.8074,\n",
      "                  'support': 5000}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.69      0.82      0.75      1746\n",
      "         dog       0.89      0.80      0.84      3254\n",
      "\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.79      0.81      0.80      5000\n",
      "weighted avg       0.82      0.81      0.81      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(paws.actual, paws.model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model 1 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.69      0.82      0.75      1746\n",
      "         dog       0.89      0.80      0.84      3254\n",
      "\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.79      0.81      0.80      5000\n",
      "weighted avg       0.82      0.81      0.81      5000\n",
      "\n",
      "None\n",
      "--------------------------\n",
      " Model 2 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.48      0.89      0.63      1746\n",
      "         dog       0.89      0.49      0.63      3254\n",
      "\n",
      "    accuracy                           0.63      5000\n",
      "   macro avg       0.69      0.69      0.63      5000\n",
      "weighted avg       0.75      0.63      0.63      5000\n",
      "\n",
      "None\n",
      "--------------------------\n",
      " Model 3 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.36      0.51      0.42      1746\n",
      "         dog       0.66      0.51      0.57      3254\n",
      "\n",
      "    accuracy                           0.51      5000\n",
      "   macro avg       0.51      0.51      0.50      5000\n",
      "weighted avg       0.55      0.51      0.52      5000\n",
      "\n",
      "None\n",
      "--------------------------\n",
      " Model 4 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.81      0.35      0.48      1746\n",
      "         dog       0.73      0.96      0.83      3254\n",
      "\n",
      "    accuracy                           0.74      5000\n",
      "   macro avg       0.77      0.65      0.66      5000\n",
      "weighted avg       0.76      0.74      0.71      5000\n",
      "\n",
      "None\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,4):\n",
    "    print(f\" Model {i + 1} \")\n",
    "    print(print(sklearn.metrics.classification_report(paws.actual, paws[paws.columns[1 + i]])))\n",
    "    print(\"--------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
